{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN PTAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import common\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import ptan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = common.HYPERPARAMS['pong']\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(params.env_name)\n",
    "env = ptan.common.wrappers.wrap_dqn(env)\n",
    "env.seed(common.SEED);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = common.DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "tgt_net = ptan.agent.TargetNet(net)\n",
    "\n",
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=params.epsilon_start)\n",
    "epsilon_tracker = common.EpsilonTracker(selector, params)\n",
    "agent = ptan.agent.DQNAgent(net, selector, device=device)\n",
    "\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, params.gamma)\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, params.replay_size)\n",
    "opt = optim.Adam(net.parameters(), params.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_batch(batch):\n",
    "    states, actions, rewards, last_states = list(zip(*batch))\n",
    "\n",
    "    states = [np.array(state, copy=False) for state in states]\n",
    "\n",
    "    last_states = torch.tensor([np.array(last_state, copy=False) if last_state is not None else states[0] for last_state in last_states]).to(device)\n",
    "\n",
    "    states = torch.tensor(states).to(device)\n",
    "    actions = torch.tensor(actions).to(device)\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "    dones = torch.BoolTensor([last_state is None for last_state in last_states])\n",
    "    \n",
    "    return states, actions, rewards, last_states, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_batch_(batch):\n",
    "    states, actions, rewards, dones, last_states = [],[],[],[],[]\n",
    "    for exp in batch:\n",
    "        state = np.array(exp.state)\n",
    "        states.append(state)\n",
    "        actions.append(exp.action)\n",
    "        rewards.append(exp.reward)\n",
    "        dones.append(exp.last_state is None)\n",
    "        if exp.last_state is None:\n",
    "            lstate = state  # the result will be masked anyway\n",
    "        else:\n",
    "            lstate = np.array(exp.last_state)\n",
    "        last_states.append(lstate)\n",
    "    return np.array(states, copy=False), np.array(actions), \\\n",
    "           np.array(rewards, dtype=np.float32), \\\n",
    "           np.array(dones, dtype=np.uint8), \\\n",
    "           np.array(last_states, copy=False)\n",
    "\n",
    "def unpack_batch(batch):\n",
    "    states, actions, rewards, dones, last_states = [],[],[],[],[]\n",
    "    for exp in batch:\n",
    "        state = np.array(exp.state)\n",
    "        states.append(state)\n",
    "        actions.append(exp.action)\n",
    "        rewards.append(exp.reward)\n",
    "        dones.append(exp.last_state is None)\n",
    "        if exp.last_state is None:\n",
    "            lstate = state  # the result will be masked anyway\n",
    "        else:\n",
    "            lstate = np.array(exp.last_state)\n",
    "        last_states.append(lstate)\n",
    "    \n",
    "    states = np.array(states, copy=False)\n",
    "    #actions = np.array(actions)\n",
    "    #rewards = np.array(rewards, dtype=np.float32)\n",
    "    #dones = np.array(dones, dtype=np.uint8)\n",
    "    next_states = np.array(last_states, copy=False)\n",
    "    \n",
    "    states_v = torch.tensor(states).to(device)\n",
    "    next_states_v = torch.tensor(next_states).to(device)\n",
    "    actions_v = torch.tensor(actions).to(device)\n",
    "    rewards_v = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "    done_mask = torch.BoolTensor(dones).to(device)\n",
    "    \n",
    "    return states_v, actions_v, rewards_v, next_states_v, done_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done populating\n",
      "Episode: 1 | Reward: -20.0 | Epsilon: 1.0 | Elapsed: 28.728986263275146\n",
      "Episode: 2 | Reward: -21.0 | Epsilon: 1.0 | Elapsed: 1.430511474609375e-06\n",
      "Episode: 3 | Reward: -20.0 | Epsilon: 1.0 | Elapsed: 9.5367431640625e-07\n",
      "Episode: 4 | Reward: -20.0 | Epsilon: 1.0 | Elapsed: 1.1920928955078125e-06\n",
      "Episode: 5 | Reward: -21.0 | Epsilon: 1.0 | Elapsed: 9.5367431640625e-07\n",
      "Episode: 6 | Reward: -21.0 | Epsilon: 1.0 | Elapsed: 1.1920928955078125e-06\n",
      "Episode: 7 | Reward: -21.0 | Epsilon: 1.0 | Elapsed: 1.1920928955078125e-06\n",
      "Episode: 8 | Reward: -21.0 | Epsilon: 1.0 | Elapsed: 7.152557373046875e-07\n",
      "Episode: 9 | Reward: -21.0 | Epsilon: 1.0 | Elapsed: 7.152557373046875e-07\n",
      "Episode: 10 | Reward: -19.0 | Epsilon: 1.0 | Elapsed: 7.152557373046875e-07\n",
      "Episode: 11 | Reward: -21.0 | Epsilon: 1.0 | Elapsed: 7.152557373046875e-07\n",
      "Episode: 12 | Reward: -20.0 | Epsilon: 0.99202 | Elapsed: 12.264180898666382\n",
      "Sync\n",
      "Episode: 13 | Reward: -21.0 | Epsilon: 0.98379 | Elapsed: 13.927971124649048\n",
      "Sync\n",
      "Episode: 14 | Reward: -21.0 | Epsilon: 0.97509 | Elapsed: 13.345125436782837\n",
      "Sync\n",
      "Episode: 15 | Reward: -19.0 | Epsilon: 0.96484 | Elapsed: 15.336654663085938\n",
      "Sync\n",
      "Episode: 16 | Reward: -20.0 | Epsilon: 0.95561 | Elapsed: 13.92359733581543\n",
      "Sync\n",
      "Episode: 17 | Reward: -21.0 | Epsilon: 0.9475 | Elapsed: 12.615367412567139\n",
      "Sync\n",
      "Episode: 18 | Reward: -20.0 | Epsilon: 0.9377 | Elapsed: 16.256505012512207\n",
      "Sync\n",
      "Episode: 19 | Reward: -19.0 | Epsilon: 0.92716 | Elapsed: 16.843783140182495\n",
      "Sync\n",
      "Episode: 20 | Reward: -20.0 | Epsilon: 0.91744 | Elapsed: 15.588517665863037\n",
      "Sync\n",
      "Episode: 21 | Reward: -21.0 | Epsilon: 0.90933 | Elapsed: 12.838428020477295\n",
      "Sync\n",
      "Episode: 22 | Reward: -19.0 | Epsilon: 0.89691 | Elapsed: 19.696718454360962\n",
      "Sync\n",
      "Episode: 23 | Reward: -20.0 | Epsilon: 0.88706 | Elapsed: 15.108154058456421\n",
      "Sync\n",
      "Episode: 24 | Reward: -21.0 | Epsilon: 0.87769 | Elapsed: 14.092039346694946\n",
      "Sync\n",
      "Episode: 25 | Reward: -21.0 | Epsilon: 0.86985 | Elapsed: 11.978636026382446\n",
      "Sync\n",
      "Episode: 26 | Reward: -19.0 | Epsilon: 0.8587 | Elapsed: 17.648868560791016\n",
      "Episode: 27 | Reward: -21.0 | Epsilon: 0.8510800000000001 | Elapsed: 11.878293991088867\n",
      "Sync\n",
      "Episode: 28 | Reward: -19.0 | Epsilon: 0.84073 | Elapsed: 16.41117262840271\n",
      "Sync\n",
      "Sync\n",
      "Episode: 29 | Reward: -19.0 | Epsilon: 0.82997 | Elapsed: 17.082830667495728\n",
      "Episode: 30 | Reward: -21.0 | Epsilon: 0.82175 | Elapsed: 13.499945163726807\n",
      "Sync\n",
      "Episode: 31 | Reward: -19.0 | Epsilon: 0.81154 | Elapsed: 15.781159162521362\n",
      "Sync\n",
      "Episode: 32 | Reward: -20.0 | Epsilon: 0.80229 | Elapsed: 13.963664770126343\n",
      "Sync\n",
      "Episode: 33 | Reward: -20.0 | Epsilon: 0.79276 | Elapsed: 14.581415891647339\n",
      "Sync\n",
      "Episode: 34 | Reward: -21.0 | Epsilon: 0.78457 | Elapsed: 13.779003858566284\n",
      "Sync\n",
      "Episode: 35 | Reward: -19.0 | Epsilon: 0.77546 | Elapsed: 14.068268775939941\n",
      "Sync\n",
      "Episode: 36 | Reward: -19.0 | Epsilon: 0.76483 | Elapsed: 16.40591788291931\n",
      "Sync\n",
      "Episode: 37 | Reward: -20.0 | Epsilon: 0.7545 | Elapsed: 16.327412366867065\n",
      "Sync\n",
      "Episode: 38 | Reward: -21.0 | Epsilon: 0.7466200000000001 | Elapsed: 12.546589851379395\n",
      "Sync\n",
      "Episode: 39 | Reward: -21.0 | Epsilon: 0.73901 | Elapsed: 11.798727989196777\n",
      "Episode: 40 | Reward: -20.0 | Epsilon: 0.73037 | Elapsed: 13.94201397895813\n",
      "Sync\n",
      "Episode: 41 | Reward: -20.0 | Epsilon: 0.7207600000000001 | Elapsed: 14.776957988739014\n",
      "Sync\n",
      "Episode: 42 | Reward: -21.0 | Epsilon: 0.71289 | Elapsed: 12.328754186630249\n",
      "Sync\n",
      "Episode: 43 | Reward: -20.0 | Epsilon: 0.7039599999999999 | Elapsed: 13.677366495132446\n",
      "Sync\n",
      "Episode: 44 | Reward: -20.0 | Epsilon: 0.6933400000000001 | Elapsed: 16.884284257888794\n",
      "Sync\n",
      "Episode: 45 | Reward: -21.0 | Epsilon: 0.6851400000000001 | Elapsed: 12.913465023040771\n",
      "Sync\n",
      "Episode: 46 | Reward: -21.0 | Epsilon: 0.6763399999999999 | Elapsed: 13.681328535079956\n",
      "Sync\n",
      "Episode: 47 | Reward: -20.0 | Epsilon: 0.66568 | Elapsed: 16.297200679779053\n",
      "Sync\n",
      "Episode: 48 | Reward: -21.0 | Epsilon: 0.65606 | Elapsed: 15.40321135520935\n",
      "Sync\n",
      "Episode: 49 | Reward: -20.0 | Epsilon: 0.64567 | Elapsed: 16.172479152679443\n",
      "Sync\n",
      "Episode: 50 | Reward: -21.0 | Epsilon: 0.63626 | Elapsed: 14.763601779937744\n",
      "Sync\n",
      "Episode: 51 | Reward: -21.0 | Epsilon: 0.62805 | Elapsed: 12.997344493865967\n",
      "Sync\n",
      "Episode: 52 | Reward: -20.0 | Epsilon: 0.6184700000000001 | Elapsed: 16.708861112594604\n",
      "Sync\n",
      "Episode: 53 | Reward: -20.0 | Epsilon: 0.60912 | Elapsed: 16.041226625442505\n",
      "Sync\n",
      "Episode: 54 | Reward: -17.0 | Epsilon: 0.59822 | Elapsed: 16.808128833770752\n",
      "Sync\n",
      "Episode: 55 | Reward: -20.0 | Epsilon: 0.58899 | Elapsed: 14.42856502532959\n",
      "Sync\n",
      "Episode: 56 | Reward: -20.0 | Epsilon: 0.57925 | Elapsed: 15.221042156219482\n",
      "Episode: 57 | Reward: -21.0 | Epsilon: 0.5714600000000001 | Elapsed: 12.01471757888794\n",
      "Sync\n",
      "Episode: 58 | Reward: -19.0 | Epsilon: 0.5604 | Elapsed: 19.08364510536194\n",
      "Sync\n",
      "Episode: 59 | Reward: -20.0 | Epsilon: 0.5509 | Elapsed: 14.726757049560547\n",
      "Sync\n",
      "Episode: 60 | Reward: -19.0 | Epsilon: 0.5403 | Elapsed: 17.04528522491455\n",
      "Sync\n",
      "Episode: 61 | Reward: -20.0 | Epsilon: 0.53104 | Elapsed: 14.649237632751465\n",
      "Sync\n",
      "Episode: 62 | Reward: -21.0 | Epsilon: 0.52226 | Elapsed: 13.086059808731079\n",
      "Sync\n",
      "Episode: 63 | Reward: -19.0 | Epsilon: 0.51249 | Elapsed: 15.729135751724243\n",
      "Sync\n",
      "Episode: 64 | Reward: -21.0 | Epsilon: 0.50249 | Elapsed: 15.775346517562866\n",
      "Sync\n",
      "Episode: 65 | Reward: -21.0 | Epsilon: 0.49341 | Elapsed: 14.221578598022461\n",
      "Sync\n",
      "Episode: 66 | Reward: -20.0 | Epsilon: 0.48441999999999996 | Elapsed: 13.827712297439575\n",
      "Sync\n",
      "Episode: 67 | Reward: -20.0 | Epsilon: 0.47399 | Elapsed: 16.77422547340393\n",
      "Sync\n",
      "Episode: 68 | Reward: -21.0 | Epsilon: 0.46614999999999995 | Elapsed: 11.802927255630493\n",
      "Sync\n",
      "Episode: 69 | Reward: -21.0 | Epsilon: 0.45655999999999997 | Elapsed: 15.227885484695435\n",
      "Sync\n",
      "Episode: 70 | Reward: -21.0 | Epsilon: 0.44704 | Elapsed: 16.32886838912964\n",
      "Sync\n",
      "Episode: 71 | Reward: -20.0 | Epsilon: 0.43667999999999996 | Elapsed: 16.5460307598114\n",
      "Sync\n",
      "Episode: 72 | Reward: -19.0 | Epsilon: 0.42618999999999996 | Elapsed: 16.421492099761963\n",
      "Sync\n",
      "Episode: 73 | Reward: -19.0 | Epsilon: 0.41447 | Elapsed: 18.18134832382202\n",
      "Sync\n",
      "Episode: 74 | Reward: -20.0 | Epsilon: 0.40437 | Elapsed: 15.40773057937622\n",
      "Sync\n",
      "Episode: 75 | Reward: -18.0 | Epsilon: 0.39365000000000006 | Elapsed: 17.58619213104248\n",
      "Sync\n",
      "Episode: 76 | Reward: -21.0 | Epsilon: 0.38438000000000005 | Elapsed: 14.71422266960144\n",
      "Sync\n",
      "Episode: 77 | Reward: -19.0 | Epsilon: 0.37083999999999995 | Elapsed: 20.922343969345093\n",
      "Sync\n",
      "Episode: 78 | Reward: -20.0 | Epsilon: 0.36251 | Elapsed: 13.428195238113403\n",
      "Sync\n",
      "Episode: 79 | Reward: -21.0 | Epsilon: 0.35261 | Elapsed: 16.08022403717041\n",
      "Sync\n",
      "Episode: 80 | Reward: -19.0 | Epsilon: 0.34052000000000004 | Elapsed: 18.534053087234497\n",
      "Sync\n",
      "Episode: 81 | Reward: -20.0 | Epsilon: 0.33070999999999995 | Elapsed: 15.39407753944397\n",
      "Sync\n",
      "Episode: 82 | Reward: -20.0 | Epsilon: 0.32120000000000004 | Elapsed: 14.690077304840088\n",
      "Sync\n",
      "Sync\n",
      "Episode: 83 | Reward: -20.0 | Epsilon: 0.30928999999999995 | Elapsed: 18.838446855545044\n",
      "Sync\n",
      "Episode: 84 | Reward: -19.0 | Epsilon: 0.29874999999999996 | Elapsed: 16.523332357406616\n",
      "Episode: 85 | Reward: -21.0 | Epsilon: 0.2903 | Elapsed: 13.011998653411865\n",
      "Sync\n",
      "Episode: 86 | Reward: -19.0 | Epsilon: 0.28032 | Elapsed: 14.88722014427185\n",
      "Sync\n",
      "Sync\n",
      "Episode: 87 | Reward: -18.0 | Epsilon: 0.26651 | Elapsed: 22.015653371810913\n",
      "Sync\n",
      "Episode: 88 | Reward: -18.0 | Epsilon: 0.25146 | Elapsed: 24.08449935913086\n",
      "Sync\n",
      "Episode: 89 | Reward: -19.0 | Epsilon: 0.24146 | Elapsed: 16.182223558425903\n",
      "Sync\n",
      "Sync\n",
      "Episode: 90 | Reward: -17.0 | Epsilon: 0.22719999999999996 | Elapsed: 22.629590034484863\n",
      "Sync\n",
      "Episode: 91 | Reward: -16.0 | Epsilon: 0.21113000000000004 | Elapsed: 25.046858310699463\n",
      "Sync\n",
      "Sync\n",
      "Episode: 92 | Reward: -18.0 | Epsilon: 0.19704999999999995 | Elapsed: 22.870856285095215\n",
      "Sync\n",
      "Sync\n",
      "Episode: 93 | Reward: -16.0 | Epsilon: 0.17874999999999996 | Elapsed: 28.942599296569824\n",
      "Sync\n",
      "Episode: 94 | Reward: -17.0 | Epsilon: 0.16180000000000005 | Elapsed: 27.307241678237915\n",
      "Sync\n",
      "Sync\n",
      "Episode: 95 | Reward: -19.0 | Epsilon: 0.14578999999999998 | Elapsed: 25.838741779327393\n",
      "Sync\n",
      "Episode: 96 | Reward: -19.0 | Epsilon: 0.13227 | Elapsed: 21.03218984603882\n",
      "Sync\n",
      "Sync\n",
      "Episode: 97 | Reward: -20.0 | Epsilon: 0.11663000000000001 | Elapsed: 23.721386671066284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sync\n",
      "Episode: 98 | Reward: -20.0 | Epsilon: 0.10297 | Elapsed: 20.961503744125366\n",
      "Sync\n",
      "Episode: 99 | Reward: -19.0 | Epsilon: 0.09058999999999995 | Elapsed: 19.509050130844116\n",
      "Sync\n",
      "Sync\n",
      "Episode: 100 | Reward: -19.0 | Epsilon: 0.07394 | Elapsed: 26.57668900489807\n",
      "Sync\n",
      "Sync\n",
      "Episode: 101 | Reward: -18.0 | Epsilon: 0.056540000000000035 | Elapsed: 27.741881132125854\n",
      "Sync\n",
      "Episode: 102 | Reward: -20.0 | Epsilon: 0.04274 | Elapsed: 21.665473222732544\n",
      "Sync\n",
      "Sync\n",
      "Episode: 103 | Reward: -21.0 | Epsilon: 0.029610000000000025 | Elapsed: 19.849308490753174\n",
      "Sync\n",
      "Episode: 104 | Reward: -19.0 | Epsilon: 0.02 | Elapsed: 21.092462301254272\n",
      "Sync\n",
      "Episode: 105 | Reward: -19.0 | Epsilon: 0.02 | Elapsed: 23.86977744102478\n",
      "Sync\n",
      "Sync\n",
      "Episode: 106 | Reward: -21.0 | Epsilon: 0.02 | Elapsed: 19.050861835479736\n",
      "Sync\n",
      "Episode: 107 | Reward: -17.0 | Epsilon: 0.02 | Elapsed: 23.99392318725586\n",
      "Sync\n",
      "Episode: 108 | Reward: -19.0 | Epsilon: 0.02 | Elapsed: 18.472620964050293\n",
      "Sync\n",
      "Sync\n",
      "Episode: 109 | Reward: -17.0 | Epsilon: 0.02 | Elapsed: 30.869173049926758\n",
      "Sync\n",
      "Sync\n",
      "Episode: 110 | Reward: -18.0 | Epsilon: 0.02 | Elapsed: 29.29362988471985\n",
      "Sync\n",
      "Sync\n",
      "Episode: 111 | Reward: -20.0 | Epsilon: 0.02 | Elapsed: 24.83553409576416\n",
      "Sync\n",
      "Episode: 112 | Reward: -19.0 | Epsilon: 0.02 | Elapsed: 24.278323650360107\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 113 | Reward: -16.0 | Epsilon: 0.02 | Elapsed: 38.783764600753784\n",
      "Sync\n",
      "Sync\n",
      "Episode: 114 | Reward: -15.0 | Epsilon: 0.02 | Elapsed: 34.96859550476074\n",
      "Sync\n",
      "Sync\n",
      "Episode: 115 | Reward: -19.0 | Epsilon: 0.02 | Elapsed: 26.937018632888794\n",
      "Sync\n",
      "Sync\n",
      "Episode: 116 | Reward: -16.0 | Epsilon: 0.02 | Elapsed: 31.61887764930725\n",
      "Sync\n",
      "Sync\n",
      "Episode: 117 | Reward: -11.0 | Epsilon: 0.02 | Elapsed: 42.8268404006958\n",
      "Sync\n",
      "Sync\n",
      "Episode: 118 | Reward: -19.0 | Epsilon: 0.02 | Elapsed: 34.30348992347717\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 119 | Reward: -14.0 | Epsilon: 0.02 | Elapsed: 42.87790083885193\n",
      "Sync\n",
      "Sync\n",
      "Episode: 120 | Reward: -20.0 | Epsilon: 0.02 | Elapsed: 26.83164405822754\n",
      "Sync\n",
      "Sync\n",
      "Episode: 121 | Reward: -15.0 | Epsilon: 0.02 | Elapsed: 33.7648708820343\n",
      "Sync\n",
      "Sync\n",
      "Episode: 122 | Reward: -19.0 | Epsilon: 0.02 | Elapsed: 27.620306968688965\n",
      "Sync\n",
      "Sync\n",
      "Episode: 123 | Reward: -18.0 | Epsilon: 0.02 | Elapsed: 40.36381435394287\n",
      "Sync\n",
      "Sync\n",
      "Episode: 124 | Reward: -17.0 | Epsilon: 0.02 | Elapsed: 33.04842257499695\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 125 | Reward: -18.0 | Epsilon: 0.02 | Elapsed: 36.032859802246094\n",
      "Sync\n",
      "Sync\n",
      "Episode: 126 | Reward: -17.0 | Epsilon: 0.02 | Elapsed: 35.822394371032715\n",
      "Sync\n",
      "Sync\n",
      "Episode: 127 | Reward: -19.0 | Epsilon: 0.02 | Elapsed: 39.10544514656067\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 128 | Reward: -15.0 | Epsilon: 0.02 | Elapsed: 40.89061164855957\n",
      "Sync\n",
      "Sync\n",
      "Episode: 129 | Reward: -15.0 | Epsilon: 0.02 | Elapsed: 37.879098892211914\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 130 | Reward: -13.0 | Epsilon: 0.02 | Elapsed: 40.43527412414551\n",
      "Sync\n",
      "Sync\n",
      "Episode: 131 | Reward: -17.0 | Epsilon: 0.02 | Elapsed: 34.87869429588318\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 132 | Reward: -17.0 | Epsilon: 0.02 | Elapsed: 41.026962995529175\n",
      "Sync\n",
      "Sync\n",
      "Episode: 133 | Reward: -17.0 | Epsilon: 0.02 | Elapsed: 42.625925064086914\n",
      "Sync\n",
      "Sync\n",
      "Episode: 134 | Reward: -19.0 | Epsilon: 0.02 | Elapsed: 24.507742643356323\n",
      "Sync\n",
      "Sync\n",
      "Episode: 135 | Reward: -18.0 | Epsilon: 0.02 | Elapsed: 31.57811450958252\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 136 | Reward: -18.0 | Epsilon: 0.02 | Elapsed: 41.287797689437866\n",
      "Sync\n",
      "Sync\n",
      "Episode: 137 | Reward: -15.0 | Epsilon: 0.02 | Elapsed: 39.89075231552124\n",
      "Sync\n",
      "Sync\n",
      "Episode: 138 | Reward: -19.0 | Epsilon: 0.02 | Elapsed: 28.105724573135376\n",
      "Sync\n",
      "Sync\n",
      "Episode: 139 | Reward: -13.0 | Epsilon: 0.02 | Elapsed: 36.90403509140015\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 140 | Reward: -16.0 | Epsilon: 0.02 | Elapsed: 50.97312951087952\n",
      "Sync\n",
      "Sync\n",
      "Episode: 141 | Reward: -16.0 | Epsilon: 0.02 | Elapsed: 37.85432291030884\n",
      "Sync\n",
      "Sync\n",
      "Episode: 142 | Reward: -14.0 | Epsilon: 0.02 | Elapsed: 40.728251934051514\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 143 | Reward: -13.0 | Epsilon: 0.02 | Elapsed: 46.32673954963684\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 144 | Reward: -16.0 | Epsilon: 0.02 | Elapsed: 40.36156368255615\n",
      "Sync\n",
      "Sync\n",
      "Episode: 145 | Reward: -19.0 | Epsilon: 0.02 | Elapsed: 38.732789039611816\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 146 | Reward: -13.0 | Epsilon: 0.02 | Elapsed: 40.00428509712219\n",
      "Sync\n",
      "Sync\n",
      "Episode: 147 | Reward: -15.0 | Epsilon: 0.02 | Elapsed: 38.616896629333496\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 148 | Reward: -11.0 | Epsilon: 0.02 | Elapsed: 45.50344920158386\n",
      "Sync\n",
      "Sync\n",
      "Episode: 149 | Reward: -15.0 | Epsilon: 0.02 | Elapsed: 31.297312021255493\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 150 | Reward: -16.0 | Epsilon: 0.02 | Elapsed: 37.961097240448\n",
      "Sync\n",
      "Sync\n",
      "Episode: 151 | Reward: -12.0 | Epsilon: 0.02 | Elapsed: 41.08337950706482\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 152 | Reward: -14.0 | Epsilon: 0.02 | Elapsed: 39.499409675598145\n",
      "Sync\n",
      "Episode: 153 | Reward: -20.0 | Epsilon: 0.02 | Elapsed: 21.287925720214844\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 154 | Reward: -11.0 | Epsilon: 0.02 | Elapsed: 44.255879163742065\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 155 | Reward: -16.0 | Epsilon: 0.02 | Elapsed: 46.81485557556152\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 156 | Reward: -13.0 | Epsilon: 0.02 | Elapsed: 46.16588068008423\n",
      "Sync\n",
      "Sync\n",
      "Episode: 157 | Reward: -11.0 | Epsilon: 0.02 | Elapsed: 65.65862369537354\n",
      "Sync\n",
      "Sync\n",
      "Episode: 158 | Reward: -16.0 | Epsilon: 0.02 | Elapsed: 39.424808502197266\n",
      "Sync\n",
      "Sync\n",
      "Episode: 159 | Reward: -16.0 | Epsilon: 0.02 | Elapsed: 34.10303020477295\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 160 | Reward: -6.0 | Epsilon: 0.02 | Elapsed: 52.530450105667114\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 161 | Reward: -13.0 | Epsilon: 0.02 | Elapsed: 58.487568855285645\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 162 | Reward: -17.0 | Epsilon: 0.02 | Elapsed: 50.250380516052246\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 163 | Reward: -15.0 | Epsilon: 0.02 | Elapsed: 50.061928272247314\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 164 | Reward: -14.0 | Epsilon: 0.02 | Elapsed: 55.78560447692871\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 165 | Reward: -4.0 | Epsilon: 0.02 | Elapsed: 73.2324481010437\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 166 | Reward: -13.0 | Epsilon: 0.02 | Elapsed: 59.24849557876587\n",
      "Sync\n",
      "Sync\n",
      "Episode: 167 | Reward: -13.0 | Epsilon: 0.02 | Elapsed: 41.77558135986328\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 168 | Reward: -11.0 | Epsilon: 0.02 | Elapsed: 51.52760720252991\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 169 | Reward: -10.0 | Epsilon: 0.02 | Elapsed: 65.29993510246277\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 170 | Reward: -12.0 | Epsilon: 0.02 | Elapsed: 67.4285101890564\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 171 | Reward: -13.0 | Epsilon: 0.02 | Elapsed: 55.76372408866882\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 172 | Reward: -3.0 | Epsilon: 0.02 | Elapsed: 77.80474781990051\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 173 | Reward: -17.0 | Epsilon: 0.02 | Elapsed: 42.95134091377258\n",
      "Sync\n",
      "Episode: 174 | Reward: -20.0 | Epsilon: 0.02 | Elapsed: 32.43891501426697\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 175 | Reward: -16.0 | Epsilon: 0.02 | Elapsed: 44.784626960754395\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 176 | Reward: -14.0 | Epsilon: 0.02 | Elapsed: 66.36657691001892\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 177 | Reward: -4.0 | Epsilon: 0.02 | Elapsed: 76.11968493461609\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 178 | Reward: -13.0 | Epsilon: 0.02 | Elapsed: 58.52371835708618\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 179 | Reward: -3.0 | Epsilon: 0.02 | Elapsed: 74.2283284664154\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 180 | Reward: -7.0 | Epsilon: 0.02 | Elapsed: 67.01931953430176\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 181 | Reward: -3.0 | Epsilon: 0.02 | Elapsed: 72.55765748023987\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 182 | Reward: -5.0 | Epsilon: 0.02 | Elapsed: 70.74201393127441\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 183 | Reward: -9.0 | Epsilon: 0.02 | Elapsed: 61.539387226104736\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 184 | Reward: -8.0 | Epsilon: 0.02 | Elapsed: 65.59018397331238\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 185 | Reward: -4.0 | Epsilon: 0.02 | Elapsed: 61.87520456314087\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 186 | Reward: -8.0 | Epsilon: 0.02 | Elapsed: 53.76048302650452\n",
      "Sync\n",
      "Sync\n",
      "Episode: 187 | Reward: -16.0 | Epsilon: 0.02 | Elapsed: 42.37200093269348\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 188 | Reward: -11.0 | Epsilon: 0.02 | Elapsed: 54.22473359107971\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 189 | Reward: -9.0 | Epsilon: 0.02 | Elapsed: 62.40041637420654\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 190 | Reward: -4.0 | Epsilon: 0.02 | Elapsed: 68.17911124229431\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 191 | Reward: -7.0 | Epsilon: 0.02 | Elapsed: 61.843703269958496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 192 | Reward: -8.0 | Epsilon: 0.02 | Elapsed: 65.51734590530396\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 193 | Reward: -3.0 | Epsilon: 0.02 | Elapsed: 78.6270318031311\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 194 | Reward: -8.0 | Epsilon: 0.02 | Elapsed: 65.26312923431396\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 195 | Reward: -8.0 | Epsilon: 0.02 | Elapsed: 57.98408532142639\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 196 | Reward: -5.0 | Epsilon: 0.02 | Elapsed: 67.9667272567749\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Sync\n",
      "Episode: 197 | Reward: 3.0 | Epsilon: 0.02 | Elapsed: 69.95203995704651\n",
      "Sync\n",
      "Sync\n",
      "Sync\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ac6005b3329b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual_qs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mepsilon_tracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastai/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastai/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'betas'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    109\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/fastai/lib/python3.8/site-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "episode = 0\n",
    "time_begin = time.time()\n",
    "\n",
    "for batch in common.batch_generator(buffer, params.replay_initial, params.batch_size):\n",
    "    iteration += 1\n",
    "    \n",
    "    for reward,steps in exp_source.pop_rewards_steps():\n",
    "        episode += 1\n",
    "        elapsed = time.time() - time_begin\n",
    "        print(f'Episode: {episode} | Reward: {reward} | Epsilon: {selector.epsilon} | Elapsed: {elapsed}')\n",
    "        time_begin = time.time()\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    states, actions, rewards, last_states, dones = unpack_batch(batch)\n",
    "\n",
    "    actual_qs = net(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "    # target\n",
    "    with torch.no_grad():\n",
    "        target_qs = tgt_net.model(last_states).max(dim=1)[0]\n",
    "        target_qs[dones] = 0.0\n",
    "        target = rewards + params.gamma*target_qs.detach()\n",
    "\n",
    "    loss = F.mse_loss(target, actual_qs)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    epsilon_tracker.frame(iteration)\n",
    "    \n",
    "    if iteration%params.target_net_sync==0:\n",
    "        print(\"Sync\")\n",
    "        tgt_net.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
