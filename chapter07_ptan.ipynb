{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ptan\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArgMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3],\n",
       "       [ 1, -1,  0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_vals = np.array([[1,2,3], [1,-1,0]])\n",
    "q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = ptan.actions.ArgmaxActionSelector()\n",
    "selector(q_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.take_along_axis(q_vals, selector(q_vals)[:,None], 1).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EpsilonGreedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=0.0)\n",
    "selector(q_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=0.5)\n",
    "selector(q_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 1]\n",
      "[1 2 0]\n",
      "[1 2 0]\n",
      "[0 2 0]\n",
      "[1 2 1]\n",
      "[1 2 0]\n",
      "[1 2 1]\n",
      "[1 2 1]\n",
      "[0 2 1]\n",
      "[1 2 1]\n"
     ]
    }
   ],
   "source": [
    "selector = ptan.actions.ProbabilityActionSelector()\n",
    "acts = np.array([[.125,.75,.125],\n",
    "                [0,0,1.],\n",
    "                [.5,.5,0]])\n",
    "\n",
    "for _ in range(10):\n",
    "    print(selector(acts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNNET(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.eye(x.size()[0], self.n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = DQNNET(n_actions=3)\n",
    "net(torch.zeros(2,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argmax selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ptan.actions.ArgmaxActionSelector()\n",
    "agent = ptan.agent.DQNAgent(dqn_model=net, action_selector=selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]), [None, None])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (actions for each batch), [internal state for each batch]\n",
    "\n",
    "agent(torch.zeros(2,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon-Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 2]), [None, None])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = ptan.actions.EpsilonGreedyActionSelector(.5)\n",
    "agent = ptan.agent.DQNAgent(dqn_model=net, action_selector=selector)\n",
    "\n",
    "agent(torch.zeros(2,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output_shape = (x.shape[0], self.n_actions)\n",
    "        result = torch.zeros(output_shape, dtype=torch.float32)\n",
    "        result[:,:2] = 1.\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = PolicyNet(5)\n",
    "net(torch.zeros(6,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = ptan.actions.ProbabilityActionSelector()\n",
    "agent = ptan.agent.PolicyAgent(model=net, action_selector=selector, apply_softmax=True)\n",
    "\n",
    "agent(torch.zeros(6,5))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.observation_space = gym.spaces.Discrete(n=5)\n",
    "        self.action_space = gym.spaces.Discrete(n=3)\n",
    "        self.step_index = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.step_index = 0\n",
    "        return self.step_index\n",
    "    \n",
    "    def step(self, action):        \n",
    "        if is_done := (self.step_index==10):\n",
    "            return self.step_index%self.observation_space.n, 0.0, is_done, {}\n",
    "        \n",
    "        self.step_index += 1\n",
    "        return self.step_index%self.observation_space.n, float(action), self.step_index==10, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2.0, False, {})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = ToyEnv()\n",
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DullAgent(ptan.agent.BaseAgent):\n",
    "    def __init__(self, action):\n",
    "        super().__init__()\n",
    "        self.action = action\n",
    "        \n",
    "    def __call__(self, observations, state):\n",
    "        return [self.action for _ in observations], state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExperienceSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
      "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=False))\n",
      "(Experience(state=4, action=1, reward=1.0, done=False), Experience(state=0, action=1, reward=1.0, done=False))\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n",
      "(Experience(state=3, action=1, reward=1.0, done=False), Experience(state=4, action=1, reward=1.0, done=True))\n",
      "(Experience(state=4, action=1, reward=1.0, done=True),)\n",
      "(Experience(state=0, action=1, reward=1.0, done=False), Experience(state=1, action=1, reward=1.0, done=False))\n",
      "(Experience(state=1, action=1, reward=1.0, done=False), Experience(state=2, action=1, reward=1.0, done=False))\n",
      "(Experience(state=2, action=1, reward=1.0, done=False), Experience(state=3, action=1, reward=1.0, done=False))\n"
     ]
    }
   ],
   "source": [
    "env = ToyEnv()\n",
    "agent = DullAgent(action=1)\n",
    "exp_source = ptan.experience.ExperienceSource(env=env, agent=agent, steps_count=2)\n",
    "\n",
    "for idx, exp in enumerate(exp_source):\n",
    "    if idx>12: break\n",
    "    print(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ExperienceSourceFirstLast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take n steps, record the first and last together with accumulated rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=1., steps_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExperienceFirstLast(state=0, action=1, reward=2.0, last_state=2)\n",
      "ExperienceFirstLast(state=1, action=1, reward=2.0, last_state=3)\n",
      "ExperienceFirstLast(state=2, action=1, reward=2.0, last_state=4)\n",
      "ExperienceFirstLast(state=3, action=1, reward=2.0, last_state=0)\n",
      "ExperienceFirstLast(state=4, action=1, reward=2.0, last_state=1)\n",
      "ExperienceFirstLast(state=0, action=1, reward=2.0, last_state=2)\n",
      "ExperienceFirstLast(state=1, action=1, reward=2.0, last_state=3)\n",
      "ExperienceFirstLast(state=2, action=1, reward=2.0, last_state=4)\n",
      "ExperienceFirstLast(state=3, action=1, reward=2.0, last_state=None)\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=None)\n",
      "ExperienceFirstLast(state=0, action=1, reward=2.0, last_state=2)\n",
      "ExperienceFirstLast(state=1, action=1, reward=2.0, last_state=3)\n"
     ]
    }
   ],
   "source": [
    "for e,exp in enumerate(exp_source):\n",
    "    print(exp)\n",
    "    if e>10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_source.pop_rewards_steps??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ToyEnv()\n",
    "agent = DullAgent(action=1)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=1., steps_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, buffer_size=100)\n",
    "len(buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal training loop now becomes:\n",
    "\n",
    "1. buffer.populate(1)\n",
    "2. batch = buffer.sample(BATCH_SIZE)\n",
    "3. loss + backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
      "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
      "ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n",
      "ExperienceFirstLast(state=2, action=1, reward=1.0, last_state=3)\n",
      "--------------------------\n",
      "ExperienceFirstLast(state=4, action=1, reward=1.0, last_state=0)\n",
      "ExperienceFirstLast(state=0, action=1, reward=1.0, last_state=1)\n",
      "ExperienceFirstLast(state=3, action=1, reward=1.0, last_state=4)\n",
      "ExperienceFirstLast(state=1, action=1, reward=1.0, last_state=2)\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "for step in range(6):\n",
    "    buffer.populate(1)\n",
    "    if len(buffer)<5: continue\n",
    "        \n",
    "    batch = buffer.sample(4)\n",
    "    print('\\n'.join(str(b) for b in batch))\n",
    "    print('--------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TargetNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syncing of two neural nets, either by copying or by linear interpolation of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Linear(5,3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQNNet(\n",
       "  (ff): Linear(in_features=5, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = DQNNet()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_net = ptan.agent.TargetNet(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0637, grad_fn=<SelectBackward>),\n",
       " tensor(-0.0637, grad_fn=<SelectBackward>))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.ff.weight[0][0], tgt_net.target_model.ff.weight[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9363, grad_fn=<SelectBackward>),\n",
       " tensor(-0.0637, grad_fn=<SelectBackward>))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.ff.weight[0][0] += 1\n",
    "net.ff.weight[0][0], tgt_net.target_model.ff.weight[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.9363, grad_fn=<SelectBackward>),\n",
       " tensor(0.9363, grad_fn=<SelectBackward>))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_net.sync()\n",
    "net.ff.weight[0][0], tgt_net.target_model.ff.weight[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CartPole Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "n_observations = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_input, n_output, n_hidden):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(nn.Linear(n_input, n_hidden),\n",
    "                       nn.ReLU(),\n",
    "                       nn.Linear(n_hidden, n_output),\n",
    "                       )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(n_observations, n_actions, 128)\n",
    "target_net = ptan.agent.TargetNet(net)\n",
    "\n",
    "#net(torch.tensor(env.observation_space.sample()).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ptan.actions.ArgmaxActionSelector()\n",
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=1., selector=selector)\n",
    "\n",
    "agent = ptan.agent.DQNAgent(net, selector)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=.9)\n",
    "buffer = ptan.experience.ExperienceReplayBuffer(exp_source, 1000)\n",
    "opt = optim.Adam(net.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def unpack_batch(batch, net, gamma):\n",
    "    states,actions,rewards,last_states = list(zip(*batch))\n",
    "    states = torch.tensor(states)\n",
    "    actions = torch.tensor(actions)\n",
    "    rewards = torch.tensor(rewards)\n",
    "\n",
    "    done_mask = [el is None for el in last_states]\n",
    "    last_states = [last_state if last_state is not None else np.array(states[0]) for last_state in last_states]\n",
    "    last_states = torch.tensor(last_states)\n",
    "\n",
    "    next_state_qs = net(last_states).max(dim=1)[0]\n",
    "    next_state_qs[done_mask] = 0.0\n",
    "    \n",
    "    return states, actions, rewards + gamma*next_state_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 | Reward: 37.0 | Epsilon: 0.9414801494009999\n",
      "Episode: 2 | Reward: 19.0 | Epsilon: 0.7778213593991465\n",
      "Episode: 3 | Reward: 20.0 | Epsilon: 0.6361854860638709\n",
      "Episode: 4 | Reward: 15.0 | Epsilon: 0.5471566423907612\n",
      "Episode: 5 | Reward: 15.0 | Epsilon: 0.47058664158564995\n",
      "Episode: 6 | Reward: 10.0 | Epsilon: 0.42559012338865465\n",
      "Episode: 7 | Reward: 11.0 | Epsilon: 0.38104711810454983\n",
      "Episode: 8 | Reward: 11.0 | Epsilon: 0.34116606151404244\n",
      "Episode: 9 | Reward: 13.0 | Epsilon: 0.29938039131233124\n",
      "Episode: 10 | Reward: 10.0 | Epsilon: 0.270754259511994\n",
      "Episode: 11 | Reward: 12.0 | Epsilon: 0.23999247958413436\n",
      "Episode: 12 | Reward: 11.0 | Epsilon: 0.21487444770607952\n",
      "Episode: 13 | Reward: 10.0 | Epsilon: 0.19432859888279505\n",
      "Episode: 14 | Reward: 10.0 | Epsilon: 0.17574730149117582\n",
      "Episode: 15 | Reward: 9.0 | Epsilon: 0.1605481911108965\n",
      "Episode: 16 | Reward: 9.0 | Epsilon: 0.1466635416321037\n",
      "Episode: 17 | Reward: 10.0 | Epsilon: 0.13263987810938213\n",
      "Episode: 18 | Reward: 10.0 | Epsilon: 0.11995712819347792\n",
      "Episode: 19 | Reward: 11.0 | Epsilon: 0.1074022057426376\n",
      "Episode: 20 | Reward: 11.0 | Epsilon: 0.09616130339314863\n",
      "Episode: 21 | Reward: 11.0 | Epsilon: 0.08609689350726446\n",
      "Episode: 22 | Reward: 10.0 | Epsilon: 0.07786448720191189\n",
      "Episode: 23 | Reward: 11.0 | Epsilon: 0.06971505404010997\n",
      "Episode: 24 | Reward: 12.0 | Epsilon: 0.06179436923202588\n",
      "Episode: 25 | Reward: 12.0 | Epsilon: 0.05477359404450838\n",
      "Episode: 26 | Reward: 10.0 | Epsilon: 0.04953625663766238\n",
      "Episode: 27 | Reward: 10.0 | Epsilon: 0.04479970256613776\n",
      "Episode: 28 | Reward: 10.0 | Epsilon: 0.040516047966540916\n",
      "Episode: 29 | Reward: 12.0 | Epsilon: 0.03591281197926689\n",
      "Episode: 30 | Reward: 13.0 | Epsilon: 0.03151424750681587\n",
      "Episode: 31 | Reward: 13.0 | Epsilon: 0.027654414711223735\n",
      "Episode: 32 | Reward: 13.0 | Epsilon: 0.024267330287830756\n",
      "Episode: 33 | Reward: 31.0 | Epsilon: 0.017771047742294682\n",
      "Episode: 34 | Reward: 13.0 | Epsilon: 0.015594467994581935\n",
      "Episode: 35 | Reward: 74.0 | Epsilon: 0.007412675071601228\n",
      "Episode: 36 | Reward: 109.0 | Epsilon: 0.0024786269915882293\n",
      "Episode: 37 | Reward: 110.0 | Epsilon: 0.0008205075478212754\n",
      "Episode: 38 | Reward: 14.0 | Epsilon: 0.0007128124965150752\n",
      "Episode: 39 | Reward: 38.0 | Epsilon: 0.00048653344487718976\n",
      "Episode: 40 | Reward: 41.0 | Epsilon: 0.00032222236288023367\n",
      "Episode: 41 | Reward: 35.0 | Epsilon: 0.0002266665784454152\n",
      "Episode: 42 | Reward: 37.0 | Epsilon: 0.00015627506530626313\n",
      "Episode: 43 | Reward: 27.0 | Epsilon: 0.0001191351574703475\n",
      "Episode: 44 | Reward: 11.0 | Epsilon: 0.00010666626391033817\n",
      "Episode: 45 | Reward: 14.0 | Epsilon: 9.266587013581706e-05\n",
      "Episode: 46 | Reward: 15.0 | Epsilon: 7.96980558004141e-05\n",
      "Episode: 47 | Reward: 19.0 | Epsilon: 6.584403308299988e-05\n",
      "Episode: 48 | Reward: 19.0 | Epsilon: 5.439827419996685e-05\n",
      "Episode: 49 | Reward: 16.0 | Epsilon: 4.6317833301711654e-05\n",
      "Episode: 50 | Reward: 13.0 | Epsilon: 4.064487246201368e-05\n",
      "Episode: 51 | Reward: 21.0 | Epsilon: 3.2911285932791276e-05\n",
      "Episode: 52 | Reward: 18.0 | Epsilon: 2.7464921017933e-05\n",
      "Episode: 53 | Reward: 11.0 | Epsilon: 2.4590394437549663e-05\n",
      "Episode: 54 | Reward: 10.0 | Epsilon: 2.223911194671613e-05\n",
      "Episode: 55 | Reward: 13.0 | Epsilon: 1.95152882660709e-05\n",
      "Episode: 56 | Reward: 17.0 | Epsilon: 1.6450279410809685e-05\n",
      "Episode: 57 | Reward: 13.0 | Epsilon: 1.4435466017192571e-05\n",
      "Episode: 58 | Reward: 26.0 | Epsilon: 1.1115931663042383e-05\n",
      "Episode: 59 | Reward: 23.0 | Epsilon: 8.821762143797204e-06\n",
      "Episode: 60 | Reward: 27.0 | Epsilon: 6.725206098026887e-06\n",
      "Episode: 61 | Reward: 41.0 | Epsilon: 4.453983220639178e-06\n",
      "Episode: 62 | Reward: 72.0 | Epsilon: 2.160143425355923e-06\n",
      "Episode: 63 | Reward: 106.0 | Epsilon: 7.444117421556904e-07\n",
      "Episode: 64 | Reward: 61.0 | Epsilon: 4.032367311002231e-07\n",
      "Episode: 65 | Reward: 63.0 | Epsilon: 2.1408061566420875e-07\n",
      "Episode: 66 | Reward: 87.0 | Epsilon: 8.929749478250035e-08\n",
      "Episode: 67 | Reward: 70.0 | Epsilon: 4.4187852623828924e-08\n",
      "Episode: 68 | Reward: 26.0 | Epsilon: 3.402655304082779e-08\n",
      "Episode: 69 | Reward: 66.0 | Epsilon: 1.752834044932018e-08\n",
      "Episode: 70 | Reward: 51.0 | Epsilon: 1.0498704795504087e-08\n",
      "Episode: 71 | Reward: 77.0 | Epsilon: 4.842233281124191e-09\n",
      "Episode: 72 | Reward: 65.0 | Epsilon: 2.51961019629487e-09\n",
      "Episode: 73 | Reward: 79.0 | Epsilon: 1.1389737903817633e-09\n",
      "Episode: 74 | Reward: 64.0 | Epsilon: 5.986406236083308e-10\n",
      "Episode: 75 | Reward: 77.0 | Epsilon: 2.761062061970301e-10\n",
      "Episode: 76 | Reward: 52.0 | Epsilon: 1.6372171591805247e-10\n",
      "Episode: 77 | Reward: 55.0 | Epsilon: 9.419806692782484e-11\n",
      "Episode: 78 | Reward: 45.0 | Epsilon: 5.99274429947553e-11\n",
      "Episode: 79 | Reward: 75.0 | Epsilon: 2.8201054137717366e-11\n",
      "Episode: 80 | Reward: 85.0 | Epsilon: 1.2002090110161261e-11\n",
      "Episode: 81 | Reward: 62.0 | Epsilon: 6.436339562152885e-12\n",
      "Episode: 82 | Reward: 62.0 | Epsilon: 3.4516043938265185e-12\n",
      "Episode: 83 | Reward: 37.0 | Epsilon: 2.3797054941053857e-12\n",
      "Episode: 84 | Reward: 67.0 | Epsilon: 1.213615882265526e-12\n",
      "Episode: 85 | Reward: 61.0 | Epsilon: 6.573976113795816e-13\n",
      "Episode: 86 | Reward: 99.0 | Epsilon: 2.430593806471683e-13\n",
      "Episode: 87 | Reward: 81.0 | Epsilon: 1.0768696801103554e-13\n",
      "Episode: 88 | Reward: 54.0 | Epsilon: 6.258404904622507e-14\n",
      "Episode: 89 | Reward: 51.0 | Epsilon: 3.748509208520931e-14\n",
      "Episode: 90 | Reward: 44.0 | Epsilon: 2.408835507916955e-14\n",
      "Episode: 91 | Reward: 50.0 | Epsilon: 1.4573600970260865e-14\n",
      "Episode: 92 | Reward: 158.0 | Epsilon: 2.978020072371461e-15\n",
      "Solved.\n"
     ]
    }
   ],
   "source": [
    "solved = False\n",
    "episode = 0\n",
    "step = 0\n",
    "\n",
    "while True:\n",
    "    step += 1\n",
    "    buffer.populate(1)\n",
    "    if len(buffer)<2*16:#batchsize\n",
    "        continue\n",
    "        \n",
    "    for reward,steps in exp_source.pop_rewards_steps():\n",
    "        # information about finished episodes since last call to this function\n",
    "        episode += 1\n",
    "        print(f'Episode: {episode} | Reward: {reward} | Epsilon: {selector.epsilon}')\n",
    "        if reward >150:\n",
    "            print('Solved.')\n",
    "            solved = True\n",
    "            \n",
    "    if solved: break\n",
    "    \n",
    "        \n",
    "    batch = buffer.sample(16)\n",
    "    states, actions, target_qs = unpack_batch(batch, target_net.target_model, .9)\n",
    "    \n",
    "    opt.zero_grad()\n",
    "\n",
    "    actual_qs = net(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "    loss = F.mse_loss(actual_qs, target_qs)\n",
    "    loss.backward()\n",
    "\n",
    "    opt.step()\n",
    "    selector.epsilon *= .99\n",
    "    \n",
    "    if step%10==0:\n",
    "        target_net.sync()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
